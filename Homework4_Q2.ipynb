{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCJlZVfmHCWx",
        "outputId": "2845ab21-5167-49be-d692-dfbc8a05f2f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.8/310.8 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.7/486.7 kB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "! pip install -q pyspark spark-nlp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "wu52cy9iIVS_",
        "outputId": "c54ee1fc-ea0d-4c30-dbc2-6d38d219f176"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Spark NLP version:  4.4.1\n",
            "Apache Spark version:  3.4.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://c229a02dfacf:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark NLP</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f856968f670>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import sparknlp\n",
        "\n",
        "from sparknlp.base import *\n",
        "from sparknlp.common import *\n",
        "from sparknlp.annotator import *\n",
        "from sparknlp.pretrained import *\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "spark = sparknlp.start(gpu = True)\n",
        "\n",
        "print(\"Spark NLP version: \", sparknlp.version())\n",
        "print(\"Apache Spark version: \", spark.version)\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POh1yNepI5th",
        "outputId": "e092cb19-dab5-46d6-bd18-4f97fd61f169"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-01 21:54:20--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24032125 (23M) [text/plain]\n",
            "Saving to: ‘news_category_train.csv’\n",
            "\n",
            "news_category_train 100%[===================>]  22.92M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-05-01 21:54:20 (161 MB/s) - ‘news_category_train.csv’ saved [24032125/24032125]\n",
            "\n",
            "--2023-05-01 21:54:22--  https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1504408 (1.4M) [text/plain]\n",
            "Saving to: ‘news_category_test.csv’\n",
            "\n",
            "news_category_test. 100%[===================>]   1.43M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-05-01 21:54:22 (26.6 MB/s) - ‘news_category_test.csv’ saved [1504408/1504408]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_train.csv\n",
        "! wget https://raw.githubusercontent.com/JohnSnowLabs/spark-nlp-workshop/master/tutorials/Certification_Trainings/Public/data/news_category_test.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wx2sPEeh_CHA",
        "outputId": "69d8f4ee-960c-4411-8329-7dca8fa1a4a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+--------------------------------------------------+\n",
            "|category|                                       description|\n",
            "+--------+--------------------------------------------------+\n",
            "|Business| Short sellers, Wall Street's dwindling band of...|\n",
            "|Business| Private investment firm Carlyle Group, which h...|\n",
            "|Business| Soaring crude prices plus worries about the ec...|\n",
            "|Business| Authorities have halted oil export flows from ...|\n",
            "|Business| Tearaway world oil prices, toppling records an...|\n",
            "|Business| Stocks ended slightly higher on Friday but sta...|\n",
            "|Business| Assets of the nation's retail money market mut...|\n",
            "|Business| Retail sales bounced back a bit in July, and n...|\n",
            "|Business|\" After earning a PH.D. in Sociology, Danny Baz...|\n",
            "|Business| Short sellers, Wall Street's dwindling  band o...|\n",
            "+--------+--------------------------------------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "trainDataset = spark.read.option(\"header\", True).csv(\"news_category_train.csv\")\n",
        "trainDataset.show(10, truncate=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DM_pZ0dJ0O6",
        "outputId": "f34532c3-fa0b-417b-9924-49ca42f77283"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|   World|30000|\n",
            "|Sci/Tech|30000|\n",
            "|  Sports|30000|\n",
            "|Business|30000|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "trainDataset.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TU-BmJaVJ348",
        "outputId": "e4480a08-31c2-46d9-fe92-ad9b009ecd0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+-----+\n",
            "|category|count|\n",
            "+--------+-----+\n",
            "|   World| 1900|\n",
            "|Sci/Tech| 1900|\n",
            "|  Sports| 1900|\n",
            "|Business| 1900|\n",
            "+--------+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "testDataset = spark.read.option(\"header\", True).csv(\"news_category_test.csv\")\n",
        "testDataset.groupBy(\"category\").count().orderBy(col(\"count\").desc()).show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4tzL6qvN_w0m"
      },
      "outputs": [],
      "source": [
        "#Question 2a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igl-mGPl_yZT",
        "outputId": "e2971370-8878-45cb-88fe-20aabf1c8ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sent_small_bert_L8_512 download started this may take some time.\n",
            "Approximate size to download 149.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\")\n",
        "\n",
        "bert_embed = BertSentenceEmbeddings.pretrained('sent_small_bert_L8_512').setInputCols([\"document\"]).setOutputCol(\"sentence_embeddings\")\n",
        "\n",
        "classifierDL = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"prediction\").setOutputCol(\"class\").setLabelColumn(\"category\").setMaxEpochs(5).setEnableOutputLogs(True)\n",
        "\n",
        "bert_clfpipeline = Pipeline(stages = [document, bert_embed, classifierDL])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "gulgtDitDSk-"
      },
      "outputs": [],
      "source": [
        "bert_pipelinemodel = bert_clfpipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "VJGVYYzIkPY7"
      },
      "outputs": [],
      "source": [
        "pred = bert_pipelinemodel.transform(testDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dlq67swwkpQ2",
        "outputId": "b2bcb4ac-ef34-4116-eb20-87e0999d71b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.86      0.83      0.85      1900\n",
            "    Sci/Tech       0.84      0.89      0.86      1900\n",
            "      Sports       0.95      0.97      0.96      1900\n",
            "       World       0.92      0.87      0.89      1900\n",
            "\n",
            "    accuracy                           0.89      7600\n",
            "   macro avg       0.89      0.89      0.89      7600\n",
            "weighted avg       0.89      0.89      0.89      7600\n",
            "\n",
            "0.8906578947368421\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = bert_pipelinemodel.transform(testDataset).select('category', 'description', 'class.result').toPandas()\n",
        "df['result'] = df['result'].apply(lambda x: x[0])\n",
        "\n",
        "print(classification_report(df.category, df.result))\n",
        "print(accuracy_score(df.category, df.result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "4D7iJlbQnFuU"
      },
      "outputs": [],
      "source": [
        "#Question 2b) Lemmatization and Stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpUogBVmnMDX",
        "outputId": "5f1b65c0-7a5f-4134-f7b6-7610c212f7cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False)\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\").setInputCols([\"cleanTokens\"]).setOutputCol(\"lemma\")\n",
        "\n",
        "word_embed = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"document\", \"lemma\"]).setOutputCol(\"embeddings\").setCaseSensitive(False)\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings().setInputCols([\"document\", \"embeddings\"]).setOutputCol(\"sentence_embeddings\").setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifier = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\").setLabelColumn(\"category\").setMaxEpochs(5).setEnableOutputLogs(True)\n",
        "\n",
        "bert_Pipeline = Pipeline(stages = [document, tokenizer, normalizer, stopwords_cleaner, lemmatizer, word_embed, embeddingsSentence, classsifier])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "YmR3WpTkryHR"
      },
      "outputs": [],
      "source": [
        "bert_pipelinemodel = bert_Pipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Baiqa6przvn",
        "outputId": "bfe01043-c649-408b-f4d3-c613e3cdac1a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.84      0.79      0.81      1900\n",
            "    Sci/Tech       0.80      0.86      0.83      1900\n",
            "      Sports       0.92      0.96      0.94      1900\n",
            "       World       0.89      0.85      0.87      1900\n",
            "\n",
            "    accuracy                           0.86      7600\n",
            "   macro avg       0.86      0.86      0.86      7600\n",
            "weighted avg       0.86      0.86      0.86      7600\n",
            "\n",
            "0.8640789473684211\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = bert_pipelinemodel.transform(testDataset).select('category', 'description', 'class.result').toPandas()\n",
        "df['result'] = df['result'].apply(lambda x: x[0])\n",
        "\n",
        "print(classification_report(df.category, df.result))\n",
        "print(accuracy_score(df.category, df.result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Bj6xArrRsAot"
      },
      "outputs": [],
      "source": [
        "#2b lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rvcXLWpUsC3Q",
        "outputId": "2394b6da-4e49-464d-db4d-03e53c6fec7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\").setInputCols([\"normalized\"]).setOutputCol(\"lemma\")\n",
        "\n",
        "word_embed = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"document\", \"lemma\"]).setOutputCol(\"embeddings\").setCaseSensitive(False)\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings().setInputCols([\"document\", \"embeddings\"]).setOutputCol(\"sentence_embeddings\").setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifier = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\").setLabelColumn(\"category\").setMaxEpochs(5).setEnableOutputLogs(True)\n",
        "\n",
        "bert_pipelineLemma = Pipeline(stages = [document, tokenizer, normalizer, lemmatizer, word_embed, embeddingsSentence, classsifier])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IAApVbbKsmqJ"
      },
      "outputs": [],
      "source": [
        "bert_pipelinemodelLemma = bert_pipelineLemma.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsGIgE7ys6nq",
        "outputId": "cbf78996-3557-46db-de77-a6066497570f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.82      0.83      0.83      1900\n",
            "    Sci/Tech       0.86      0.82      0.84      1900\n",
            "      Sports       0.92      0.97      0.94      1900\n",
            "       World       0.88      0.86      0.87      1900\n",
            "\n",
            "    accuracy                           0.87      7600\n",
            "   macro avg       0.87      0.87      0.87      7600\n",
            "weighted avg       0.87      0.87      0.87      7600\n",
            "\n",
            "0.8714473684210526\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = bert_pipelinemodelLemma.transform(testDataset).select('category', 'description', 'class.result').toPandas()\n",
        "df['result'] = df['result'].apply(lambda x: x[0])\n",
        "\n",
        "print(classification_report(df.category, df.result))\n",
        "print(accuracy_score(df.category, df.result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "isNRQNzZtpOr"
      },
      "outputs": [],
      "source": [
        "#2b stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbuORZ9Dtxeg",
        "outputId": "02fcd18c-3866-4bc2-cebf-6ce8ce9d3dde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert_base_cased download started this may take some time.\n",
            "Approximate size to download 389.1 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False)\n",
        "\n",
        "word_embed = BertEmbeddings.pretrained('bert_base_cased', 'en').setInputCols([\"document\", \"cleanTokens\"]).setOutputCol(\"embeddings\").setCaseSensitive(False)\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings().setInputCols([\"document\", \"embeddings\"]).setOutputCol(\"sentence_embeddings\").setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifier = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\").setLabelColumn(\"category\").setMaxEpochs(5).setEnableOutputLogs(True)\n",
        "\n",
        "bert_pipelineSW = Pipeline(stages = [document, tokenizer, normalizer, stopwords_cleaner, word_embed, embeddingsSentence, classsifier])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cONsMO0dug-a"
      },
      "outputs": [],
      "source": [
        "bert_pipelinemodelSW = bert_pipelineSW.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikCXJL2pumZX",
        "outputId": "f78e9668-e9cb-4d3a-fb4e-0f850c8f4da3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.81      0.84      0.82      1900\n",
            "    Sci/Tech       0.87      0.82      0.84      1900\n",
            "      Sports       0.94      0.94      0.94      1900\n",
            "       World       0.86      0.88      0.87      1900\n",
            "\n",
            "    accuracy                           0.87      7600\n",
            "   macro avg       0.87      0.87      0.87      7600\n",
            "weighted avg       0.87      0.87      0.87      7600\n",
            "\n",
            "0.8698684210526316\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = bert_pipelinemodelSW.transform(testDataset).select('category', 'description', 'class.result').toPandas()\n",
        "df['result'] = df['result'].apply(lambda x: x[0])\n",
        "\n",
        "print(classification_report(df.category, df.result))\n",
        "print(accuracy_score(df.category, df.result))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0VGFhZHOuyxl"
      },
      "outputs": [],
      "source": [
        "#Question 2c)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35KDqMKZu058",
        "outputId": "89f6d1d1-e529-4d2d-d5a0-b7c48b475909"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "lemma_antbnc download started this may take some time.\n",
            "Approximate size to download 907.6 KB\n",
            "[OK!]\n",
            "roberta_base download started this may take some time.\n",
            "Approximate size to download 284.8 MB\n",
            "[OK!]\n"
          ]
        }
      ],
      "source": [
        "document = DocumentAssembler().setInputCol(\"description\").setOutputCol(\"document\")\n",
        "\n",
        "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
        "\n",
        "normalizer = Normalizer().setInputCols([\"token\"]).setOutputCol(\"normalized\")\n",
        "\n",
        "stopwords_cleaner = StopWordsCleaner().setInputCols([\"normalized\"]).setOutputCol(\"cleanTokens\").setCaseSensitive(False)\n",
        "\n",
        "lemmatizer = LemmatizerModel.pretrained(\"lemma_antbnc\").setInputCols([\"cleanTokens\"]).setOutputCol(\"lemma\")\n",
        "\n",
        "word_embed = RoBertaEmbeddings.pretrained('roberta_base', 'en').setInputCols([\"document\", \"lemma\"]).setOutputCol(\"embeddings\").setCaseSensitive(False)\n",
        "\n",
        "embeddingsSentence = SentenceEmbeddings().setInputCols([\"document\", \"embeddings\"]).setOutputCol(\"sentence_embeddings\").setPoolingStrategy(\"AVERAGE\")\n",
        "\n",
        "classsifier = ClassifierDLApproach().setInputCols([\"sentence_embeddings\"]).setOutputCol(\"class\").setLabelColumn(\"category\").setMaxEpochs(5).setEnableOutputLogs(True)\n",
        "\n",
        "roberta_pipeline = Pipeline(stages = [document, tokenizer, normalizer, stopwords_cleaner, lemmatizer, word_embed, embeddingsSentence, classsifier])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "8URPRpcavuOC"
      },
      "outputs": [],
      "source": [
        "roberta_pipelinemodel = roberta_pipeline.fit(trainDataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR9GGBXsv20I",
        "outputId": "69d507b6-ab57-4afd-a5f1-392524c65a85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    Business       0.79      0.86      0.83      1900\n",
            "    Sci/Tech       0.87      0.81      0.84      1900\n",
            "      Sports       0.93      0.94      0.94      1900\n",
            "       World       0.89      0.85      0.87      1900\n",
            "\n",
            "    accuracy                           0.87      7600\n",
            "   macro avg       0.87      0.87      0.87      7600\n",
            "weighted avg       0.87      0.87      0.87      7600\n",
            "\n",
            "0.8675\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "df = roberta_pipelinemodel.transform(testDataset).select('category', 'description', 'class.result').toPandas()\n",
        "df['result'] = df['result'].apply(lambda x: x[0])\n",
        "\n",
        "print(classification_report(df.category, df.result))\n",
        "print(accuracy_score(df.category, df.result))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
